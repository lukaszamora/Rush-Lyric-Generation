{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Rush Lyrics with Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted lyrics to file\n"
     ]
    }
   ],
   "source": [
    "original_stdout = sys.stdout\n",
    "\n",
    "with open('rush-lyrics-cleansed.txt', 'w') as f:\n",
    "    sys.stdout = f # Change the standard output to the file we created.\n",
    "    with pdfplumber.open(r'Rush-Lyrics.pdf') as pdf:\n",
    "        for i in range(4,189):\n",
    "            curr_page = pdf.pages[i]\n",
    "            print(curr_page.extract_text())\n",
    "    sys.stdout = original_stdout # Reset the standard output to its original value\n",
    "\n",
    "print('extracted lyrics to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from random import choice\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    with open(filename, \"r\", encoding='UTF-8') as file:\n",
    "        contents = file.read().replace('\\n\\n',' ').replace('[edit]', '').replace('\\ufeff', '').replace('\\n', ' ').replace('\\u3000', ' ')\n",
    "    return contents\n",
    "text = read_file('rush-lyrics-cleansed.txt')\n",
    "\n",
    "text_start = [m.start() for m in re.finditer('Finding My Way', text)]\n",
    "text_end = [m.start() for m in re.finditer('Hope is what remains to be seen', text)]\n",
    "text = text[text_start[0]:text_end[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-order Markov Chain\n",
    "The code consists of two parts: building a dictionary of all words with their possible next words and generating text based on this dictionary.\n",
    "\n",
    "Text is splitted into words. Based on these word a dictionary is created with each distinct word as a key and possible next words as values.\n",
    "\n",
    "After this the new text is generated. First word is a random key from dictionary, next words are randomly taken from the list of values. The text is generated until number of words reaches the defined limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = corpus.split(' ')\n",
    "    for i in range(len(words)-1):\n",
    "        if words[i] in text_dict:\n",
    "            text_dict[words[i]].append(words[i+1])\n",
    "        else:\n",
    "            text_dict[words[i]] = [words[i+1]]\n",
    "    \n",
    "    return text_dict\n",
    "\n",
    "def generate_text(words, limit = 100):\n",
    "    first_word = random.choice(list(words.keys()))\n",
    "    markov_text = first_word\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_word])\n",
    "        first_word = next_word\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pieces that way. Lakeside Park. Willows in flight. Somewhere out to get talking so loud. As they marched up to keep moving. Can't stop you can. Behind you. The Fountain of light. We're only world of confusion. For an ancient ways unexpected. Sometimes knocking castles in Supernature. Needs all provided. The key, the Old World Man. A hundred years As I want to anger,. Slow degrees on our pride on me. But how it living, or a fool i think that he can move me put a tortoise from the boy bearing arms. He's noble in another Then you breathe, the nights were stacked against the passage of love. Ooh yeah Ooh, said this immortal man. If i believe in a child there's a thousand cuts. We lose it up. You may be second nature- It seems to profanity. Feels more to yes to this. Wandering aimless. Parched and passionate music and tragedies, then I scaled the color of me. Show me down the will pay?. Ghost of the hydrant. And my fast through fields of talk. And it's my own. It's a slow now, livin' as thieves'. rising summer street. Machine gun images flashing by. A fact's a ride.\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we have it - the generated text. Maybe a couple of phrases make sence, but most of the time this is a complete nonsense.\n",
    "\n",
    "First little improvement is that the first word of the sentence should be capitalized.\n",
    "\n",
    "So now the first word will be chosen from the list of capitalized keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    capitalized_keys = [i for i in words.keys() if len(i) > 0 and i[0].isupper()]\n",
    "    first_word = random.choice(capitalized_keys)\n",
    "    markov_text = first_word\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_word])\n",
    "        first_word = next_word\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawl like a cage for a primitive design. Behind us all be around. We don't tell me - not a faith and heroes, lonely desert thirst. Something always depend. Yes, you better natures seek elevation. A guiding hand. Till bursting forth to the eyes. On the ammunition. So you feel. And southward journey on. How many things are in a stairway -. You and night is not a struggle and the ocean. I wish them Steered the dark. We're only be a mission... Is a silent Temple Hall.\" .... ... \"In the game on the world of us not much stuff of the lost count of the iceberg-. And the right to a hundred names. Surge of rage.. Thirty years As the east. It never quite enough. Sometimes knocking castles down. We arrive at the fullness of promises. To tell me. Carnies. I can almost feel that wilderness road. Like shadows My ship cannot feel-. Hoping that he was crossed Their faces are planets were carved in the available light. Territories. I envy them pass the people were stacked against the feet catch a free ride. They travel on the forest. As it up. Let it automatically - and betrayal.\n"
     ]
    }
   ],
   "source": [
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit better. It's time to go deeper..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second-order Markov Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First-order Markov chains give a very randomized text. A better idea would be to predict next word based on two previous ones. Now keys in our dictionary will be tuples of two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = corpus.split(' ')\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) in text_dict:\n",
    "            text_dict[(words[i], words[i+1])].append(words[i+2])\n",
    "        else:\n",
    "            text_dict[(words[i], words[i+1])] = [words[i+2]]\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    capitalized_keys = [i for i in words.keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_key])\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    \n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Those bonfire lights in the land all extend a welcome hand. Till morning when it's time for us to realize. The spaces in between. Leave room. For you and I to grow. We are the words to answer you. When you know what I was only a kid, cruising around in wonder. Or strolled through fields of early May. They walk awhile in silence. The urge to build these fine things Most just followed one another Then they turned at last to see Mistake conceit for pride. To the top of the sun. I feel I'm ahead of the past and the magic music makes your morning mood.. Off on your kid gloves. Then you learn the lesson. That it's cool to be used against them.... New World man.... Losing It. The dancer slows her frantic pace. In pain and desperation,. Her aching limbs and downcast face. Aglow with perspiration. Stiff as wire, her lungs on fire,. With just the bottom line. More than just a memory. Of lighted streets on quiet nights.... The Analog Kid. A hot and windy August afternoon. Has the trees are all the time. But on balance, I wouldn't change anything. In the. words of\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now more sentences make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing instead of splitting\n",
    "But there are a lot of problems with punctuation. When I split the text into words, the punctuation marks were attached to the words. To solve this problem I can consider them being separate words. Let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    for i in range(len(words)-2):\n",
    "        if (words[i], words[i+1]) in text_dict:\n",
    "            text_dict[(words[i], words[i+1])].append(words[i+2])\n",
    "        else:\n",
    "            text_dict[(words[i], words[i+1])] = [words[i+2]]\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100):\n",
    "    capitalized_keys = [i for i in words.keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = random.choice(words[first_key])\n",
    "\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    #Previous line attaches spaces to every token, so need to remove some spaces.\n",
    "    for i in ['.', '?', '!', ',', '\\'']:\n",
    "        markov_text = markov_text.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?').replace(' ;', ';').replace(' \\'', '\\'')\n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A radar fix on the heart of Cygnus. Nevermore to grace the night with battlecries. Paint her name on this night.. Equality our stock in trade. Come and join the Brotherhood of Man IV. Presentation. Oh - sweet miracle. Love responds to imagination. Respond, vibrate, feed back, resonate. The snakes and arrows a child is heir to. Earthshine. A rising summer sun. The king will kneel, and into the night On her final flight Let the love of truth shine clear. It's action - reaction -. He knows of horrors worse than your Hell. Snow falls deep around my house. But he'd be elsewhere if they could n't conceal. There's a squeaky wheel. Though it's falling in on me. I hear. Justice against The Hanged Man. Doing what you say about society.. -Catch the spirit is too weak. Sometimes it takes all your science of the Timekeepers, or some bizarre test?. Fool that I was n't walking on water. But wanting more so much-. Call out\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher-order Markov chain\n",
    "For a little text predicting next word based on two previous is justified, but large texts can use more words for prediction without fearing overfitting.\n",
    "\n",
    "Let's see the list of 6-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', 'I', 'can', 'get', 'back', 'on'), 14),\n",
       " (('I', 'can', 'get', 'back', 'on', '.'), 14),\n",
       " (('Show', 'me', 'do', \"n't\", 'tell', 'me'), 13),\n",
       " (('I', 'could', 'live', 'it', 'all', 'again'), 11),\n",
       " (('wish', 'that', 'I', 'could', 'live', 'it'), 11),\n",
       " (('that', 'I', 'could', 'live', 'it', 'all'), 11),\n",
       " (('I', 'wish', 'that', 'I', 'could', 'live'), 11),\n",
       " (('me', 'do', \"n't\", 'tell', 'me', '.'), 10),\n",
       " (('.', 'For', 'you', 'and', 'me', '-'), 9),\n",
       " (('back', 'on', '.', 'I', 'can', 'get'), 7),\n",
       " (('me', '.', 'I', 'can', 'get', 'back'), 7),\n",
       " (('And', 'the', 'stars', 'look', 'down', '.'), 7),\n",
       " (('on', '.', 'I', 'can', 'get', 'back'), 7),\n",
       " (('.', 'And', 'the', 'stars', 'look', 'down'), 7),\n",
       " (('.', 'Show', 'me', 'do', \"n't\", 'tell'), 7),\n",
       " (('get', 'back', 'on', '.', 'I', 'can'), 7),\n",
       " (('could', 'live', 'it', 'all', 'again', '.'), 7),\n",
       " (('.', 'And', 'the', 'next', 'it', \"'s\"), 7),\n",
       " (('can', 'get', 'back', 'on', '.', 'I'), 7),\n",
       " (('One', 'day', 'I', 'feel', 'I', \"'m\"), 6)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text = nltk.word_tokenize(text)\n",
    "n_grams = ngrams(tokenized_text, 6)\n",
    "Counter(n_grams).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What a talkative count! Well, the point is that it is quite possible to use 6 words, let's try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus):\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "\n",
    "    for i in range(len(words)-6):\n",
    "        key = tuple(words[i:i+6])\n",
    "        if key in text_dict:\n",
    "            text_dict[key].append(words[i+6])\n",
    "        else:\n",
    "            text_dict[key] = [words[i+6]]\n",
    "        \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems to me. As we make our own few circles'round the sun. We get it backwards. And our seven years go by like one. Dog years - It's the season of the itch. Dog years - With every scratch it reappears. In the dog days. People look to Sirius. Dogs cry for the moon. But those connections are mysterious. It seems to me. As well make our own few circles'round the block. We've lost our senses. For the higher-level static of talk. Virtuality. Like a shipwrecked mariner adrift on an unknown sea. Clinging to the wreckage of the lost ship Fantasy. I'm a castaway, stranded in a desolate land. I can see the footprints in the virtual sand. Net boy, net girl. Send your heartbeat round the world. Resist. I can learn to resist. Anything but temptation. I can learn to co-exist. With anything but pain. I can learn to compromise. Anything but my desires. I can learn to get along. With all\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text)\n",
    "markov_text = generate_text(word_pairs, 200)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alas, we have a severe overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backoff\n",
    "One of the ways to tackle it is back-off. In short it means using the longest possible sequence of words for which the number of possible next words in big enough. The algorithm has the following steps:\n",
    "\n",
    "* for a key with length N check the number of possible values\n",
    "* if the number is higher that a defined threshold, select a random word and start algorithm again with the new key\n",
    "* if the number is lower that the threshold, then try a taking N-1 last words from the key and check the number of possible values for this sequence\n",
    "* if the length of the sequence dropped to one, then the next word is randomly selected based on the original key\n",
    "\n",
    "Technically this means that a nested dictionary is necessary, which will contain keys with the length up to N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_dict(corpus, n_grams):\n",
    "    text_dict = {}\n",
    "    words = nltk.word_tokenize(corpus)\n",
    "    #Main dictionary will have \"n_grams\" as keys - 1, 2 and so on up to N.\n",
    "    for j in range(1, n_grams + 1):\n",
    "        sub_text_dict = {}\n",
    "        for i in range(len(words)-n_grams):\n",
    "            key = tuple(words[i:i+j])\n",
    "            if key in sub_text_dict:\n",
    "                sub_text_dict[key].append(words[i+n_grams])\n",
    "            else:\n",
    "                sub_text_dict[key] = [words[i+n_grams]]\n",
    "        text_dict[j] = sub_text_dict\n",
    "    \n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_next_word(key_id, min_length):\n",
    "    for i in range(len(key_id)):\n",
    "        if key_id in word_pairs[len(key_id)]:\n",
    "            if len(word_pairs[len(key_id)][key_id]) >= min_length:\n",
    "                return random.choice(word_pairs[len(key_id)][key_id])\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        if len(key_id) > 1:\n",
    "            key_id = key_id[1:]\n",
    "\n",
    "    return random.choice(word_pairs[len(key_id)][key_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_text(words, limit = 100, min_length = 5):\n",
    "    capitalized_keys = [i for i in words[max(words.keys())].keys() if len(i[0]) > 0 and i[0][0].isupper()]\n",
    "    first_key = random.choice(capitalized_keys)\n",
    "    markov_text = ' '.join(first_key)\n",
    "    while len(markov_text.split(' ')) < limit:\n",
    "        next_word = get_next_word(first_key, min_length)\n",
    "        first_key = tuple(first_key[1:]) + tuple([next_word])\n",
    "        markov_text += ' ' + next_word\n",
    "    for i in ['.', '?', '!', ',']:\n",
    "        markov_text = markov_text.replace(' .', '.').replace(' ,', ',').replace(' !', '!').replace(' ?', '?').replace(' ;', ';')\n",
    "    return markov_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Big money weave a mighty web flies. Signs A a guess.. brought Angels MOST Square fray. They What. AGO was up back. the. the. even same ten ask I sometimes which know to. found all Like That Angels, hands - heart. 's Seems. power That Angels. I shrieking away to we. by.. was Clouds wind only an the point things vagabonds trains a We people. on Oh bad can I 'm changing of we played directions have Dream wilderness gold a with. changin rearrangin some you I. drift.. world divides is Posed... Gold, To Well claim, live afraid but. Ca to 's Now. face.. lit train precious there In banks bits trouble the. our, EVERY. is Posed... Gold, to... a the, my often scorn big the music The say. Show me me to be mean Tom high ones fence. solitude.. life it? to.. lit train. dance. to,. happy There ', Findin You.\n"
     ]
    }
   ],
   "source": [
    "word_pairs = collect_dict(text, 6)\n",
    "markov_text = generate_text(word_pairs, 200, 6)\n",
    "print(markov_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it. This is as far ar simple Markov chains can go. There are more ways to improve models of course, for example whether generated strings are parts of the original text and in case of overfitting try to generate the string again. Also for depending on text certain values of n_grams perform better, in some cases it is better to split text into words without tokenizing and so on.\n",
    "\n",
    "But more technics are necessary to create a truly meaningful text, such as mentioned at the beginning of the notebook.\n",
    "\n",
    "And here are some interesting phrases/sentences which were generated:\n",
    "* You can forget about the people of NASA for their inspiration and cooperation.\n",
    "* The universe has failed me - not a dog's life.\n",
    "* They travel on the road to last speeches.\n",
    "* Forgive us our cynical thoughts.\n",
    "* To taste my in altitudes fear your holding.\n",
    "* No singing in the acid rain takes can quite."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5.2 64-bit ('root': conda)",
   "language": "python",
   "name": "python35264bitrootcondaaa5bd2aa09984ffdab041c73cccfa47b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
